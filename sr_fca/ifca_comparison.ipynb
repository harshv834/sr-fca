{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421d8fdf-262b-449a-8fee-acac1761c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4fdcb7-79fd-48d0-8122-fd1c361dd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from abc import ABC\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import json\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7904f993-87f1-413e-a02e-7c13684b4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"seed\"] = 46\n",
    "seed = config[\"seed\"]\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dc6428-cf62-4823-a897-0353e94c12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"participation_ratio\"] = 0.05/6\n",
    "#config[\"total_num_clients_per_cluster\"] = 80\n",
    "#config[\"num_clients_per_cluster\"] = int(config[\"participation_ratio\"]*config[\"total_num_clients_per_cluster\"])\n",
    "#config[\"num_clusters\"] = 4\n",
    "#config[\"num_clients\"] = config[\"num_clients_per_cluster\"]*config[\"num_clusters\"]\n",
    "config[\"dataset\"] = \"femnist\"\n",
    "#DATASET_LIB = {\"mnist\" : torchvision.datasets.MNIST, \"emnist\": torchvision.datasets.EMNIST, \"cifar10\": torchvision.datasets.CIFAR10}\n",
    "config[\"dataset_dir\"] = \"/base_vol/femnist/data\"\n",
    "config[\"results_dir\"] = \"./experiments/results\"\n",
    "config[\"results_dir\"] = os.path.join(config[\"results_dir\"], config[\"dataset\"] + \"ifca\", \"seed_{}\".format(seed))\n",
    "os.makedirs(config[\"results_dir\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b01b13a-5504-464c-b844-9be1f88cbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def read_dir(data_dir):\n",
    "    clients = []\n",
    "    groups = []\n",
    "    data = defaultdict(lambda : None)\n",
    "\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [f for f in files if f.endswith('.json')]\n",
    "    for f in files:\n",
    "        file_path = os.path.join(data_dir,f)\n",
    "        with open(file_path, 'r') as inf:\n",
    "            cdata = json.load(inf)\n",
    "        clients.extend(cdata['users'])\n",
    "        if 'hierarchies' in cdata:\n",
    "            groups.extend(cdata['hierarchies'])\n",
    "        data.update(cdata['user_data'])\n",
    "\n",
    "    clients = list(sorted(data.keys()))\n",
    "    return clients, groups, data\n",
    "\n",
    "\n",
    "def read_data(train_data_dir, test_data_dir):\n",
    "    '''parses data in given train and test data directories\n",
    "    assumes:\n",
    "    - the data in the input directories are .json files with \n",
    "        keys 'users' and 'user_data'\n",
    "    - the set of train set users is the same as the set of test set users\n",
    "    \n",
    "    Return:\n",
    "        clients: list of client ids\n",
    "        groups: list of group ids; empty list if none found\n",
    "        train_data: dictionary of train data\n",
    "        test_data: dictionary of test data\n",
    "    '''\n",
    "    train_clients, train_groups, train_data = read_dir(train_data_dir)\n",
    "    test_clients, test_groups, test_data = read_dir(test_data_dir)\n",
    "\n",
    "    assert train_clients == test_clients\n",
    "    assert train_groups == test_groups\n",
    "\n",
    "    return train_clients, train_groups, train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59084134-2948-47a4-bda2-302d5f713e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"total_clients\"], _, train_data, test_data = read_data(os.path.join(config[\"dataset_dir\"],\"train\"), os.path.join(config[\"dataset_dir\"],\"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a1f94f-dbb4-4d5e-98c5-e0e02ffb4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientDataset(Dataset):\n",
    "    def __init__(self, data,transforms = None):\n",
    "        super(ClientDataset,self).__init__()\n",
    "        self.data = data[0]\n",
    "        self.labels = data[1]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        idx_data = self.data[idx].reshape(28,28)\n",
    "        if self.transforms is not None:\n",
    "            transformed_data =self.transforms(idx_data)\n",
    "        else:\n",
    "            transformed_data = idx_data\n",
    "        idx_labels = self.labels[idx]\n",
    "        return (transformed_data.float(), idx_labels)\n",
    "\n",
    "class Client():\n",
    "    def __init__(self, train_data, test_data, client_id,  train_transforms, test_transforms, train_batch_size, test_batch_size, save_dir):\n",
    "        self.trainset = ClientDataset(train_data, train_transforms)\n",
    "        self.testset = ClientDataset(test_data, test_transforms)\n",
    "        self.trainloader = DataLoader(self.trainset, batch_size = train_batch_size, shuffle=True, num_workers=1)\n",
    "        self.testloader = DataLoader(self.testset, batch_size = test_batch_size, shuffle=False, num_workers=1)\n",
    "        self.train_iterator = iter(self.trainloader)\n",
    "        self.test_iterator = iter(self.testloader)\n",
    "        self.client_id = client_id\n",
    "        self.save_dir = os.path.join(save_dir, \"init\", \"client_{}\".format(client_id))\n",
    "\n",
    "    def sample_batch(self, train=True):\n",
    "        iterator = self.train_iterator if train else self.test_iterator\n",
    "        try:\n",
    "            (data, labels) = next(iterator)\n",
    "        except StopIteration:\n",
    "            if train:\n",
    "                self.train_iterator = iter(self.trainloader)\n",
    "                iterator = self.train_iterator\n",
    "            else:\n",
    "                self.test_iterator = iter(self.testloader)\n",
    "                iterator = self.test_iterator\n",
    "            (data, labels) = next(iterator)\n",
    "        return (data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38d0394-e1ed-4b15-9d18-6b6728e8da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate new clients\n",
    "selected_clients_path = os.path.join(\"./experiments/results\", config[\"dataset\"], \"seed_{}\".format(config[\"seed\"]),\"selected_clients.json\")\n",
    "if os.path.exists(selected_clients_path):\n",
    "    with open(selected_clients_path, \"r\") as f:\n",
    "        config[\"selected_clients\"] = json.load(f)\n",
    "else:\n",
    "    config[\"selected_clients\"] = random.sample(config[\"total_clients\"], config[\"num_clients\"])\n",
    "    with open(selected_clients_path, \"w\") as f:\n",
    "        json.dump(config[\"selected_clients\"], f)\n",
    "with open(os.path.join(config[\"results_dir\"], \"selected_clients.json\"), \"w\") as f:\n",
    "    json.dump(config[\"selected_clients\"], f)\n",
    "random.shuffle(config[\"selected_clients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d285f2-290e-4096-bd68-a4dfd0dd347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"train_batch\"] = 64\n",
    "config[\"test_batch\"] = 512\n",
    "config[\"num_clients\"] = int(len(config[\"total_clients\"])/6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d7d54c-8559-4928-b477-ed164ea0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_loaders = []\n",
    "for client_id in config[\"selected_clients\"]:\n",
    "        client_loaders.append(\n",
    "            Client(\n",
    "                (np.array(train_data[client_id]['x']), np.array(train_data[client_id]['y'])),\n",
    "                (np.array(test_data[client_id]['x']), np.array(test_data[client_id]['y'])),\n",
    "                client_id,\n",
    "                train_transforms=torchvision.transforms.ToTensor(),\n",
    "                test_transforms=torchvision.transforms.ToTensor(),\n",
    "                train_batch_size=config[\"train_batch\"],\n",
    "                test_batch_size=config[\"test_batch\"],\n",
    "                save_dir=config[\"results_dir\"],\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5ecc05-bdae-489b-b30a-0f24e25026c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size = (5,5), padding=\"same\")\n",
    "        self.pool1 = torch.nn.MaxPool2d((2,2), stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size= (5,5), padding = \"same\")\n",
    "        self.pool2 = torch.nn.MaxPool2d((2,2), stride=2)\n",
    "        self.fc1 = torch.nn.Linear(64*7*7, 2048)\n",
    "        self.fc2 = torch.nn.Linear(2048, 62)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9151d0ef-4a80-40d8-b610-ea72bcc4fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights(model):\n",
    "    model_wt = torch.load('/base_vol/model_wt_dict.pt')\n",
    "    new_wts = OrderedDict()\n",
    "    new_wts['fc2.weight'] = torch.Tensor(model_wt[\"dense_1/kernel\"]).t()\n",
    "    new_wts['fc2.bias'] = torch.Tensor(model_wt[\"dense_1/bias\"])\n",
    "    new_wts['fc1.weight'] = torch.Tensor(model_wt[\"dense/kernel\"]).t()\n",
    "    new_wts['fc1.bias'] = torch.Tensor(model_wt[\"dense/bias\"])\n",
    "    new_wts[\"conv1.weight\"] = torch.Tensor(model_wt[\"conv2d/kernel\"]).permute(3,2,0,1)\n",
    "    new_wts[\"conv2.weight\"] = torch.Tensor(model_wt[\"conv2d_1/kernel\"]).permute(3,2,0,1)\n",
    "    new_wts[\"conv1.bias\"] = torch.Tensor(model_wt[\"conv2d/bias\"])\n",
    "    new_wts[\"conv2.bias\"] = torch.Tensor(model_wt[\"conv2d_1/bias\"])\n",
    "    model.load_state_dict(new_wts)\n",
    "    return freeze_layers(model)\n",
    "def freeze_layers(model):\n",
    "    model.conv1.weight.requires_grad =False\n",
    "    model.conv2.weight.requires_grad =False\n",
    "    model.fc1.weight.requires_grad =True\n",
    "    model.fc2.weight.requires_grad =True\n",
    "    model.conv1.bias.requires_grad =False\n",
    "    model.conv2.bias.requires_grad =False\n",
    "    model.fc1.bias.requires_grad =True\n",
    "    model.fc2.bias.requires_grad =True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10091856-8414-4f64-9d6a-08c963998e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(model, device, client_data, train):\n",
    "    loader = client_data.trainloader if train else client_data.testloader\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    num_corr = 0\n",
    "    tot_num = 0\n",
    "    with torch.no_grad():\n",
    "        for (X,Y) in loader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X).argmax(axis=1).detach().cpu()\n",
    "            num_corr += (Y == pred).float().sum()\n",
    "            tot_num += Y.shape[0]\n",
    "    acc = num_corr/tot_num\n",
    "    acc *= 100.0\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41cf455b-19eb-4360-b0c8-c7a01626ab65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3401d238-65dd-4780-8c6e-acc853f14a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer(ABC):\n",
    "    def __init__(self,config, save_dir):\n",
    "        super(BaseTrainer, self).__init__()\n",
    "        self.model = set_weights(MODEL_LIST[config[\"model\"]](**config[\"model_params\"]))\n",
    "        self.save_dir = save_dir\n",
    "        self.device = config[\"device\"]\n",
    "        self.loss_func = LOSSES[config[\"loss_func\"]]\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def test(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_model_weights(self):\n",
    "        model_path  = os.path.join(self.save_dir, \"model.pth\")\n",
    "        if os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            print(\"No model present at path : {}\".format())\n",
    "\n",
    "    def save_model_weights(self):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        model_path  = os.path.join(self.save_dir, \"model.pth\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "    def save_metrics(self, train_loss, test_acc, iteration):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        torch.save({\"train_loss\": train_loss,  \"test_acc\" : test_acc}, os.path.join(self.save_dir,\"metrics_{}.pkl\".format(iteration)))\n",
    "\n",
    "class ClusterTrainer(BaseTrainer):\n",
    "    def __init__(self,  config, save_dir, cluster_id):\n",
    "        super(ClusterTrainer, self).__init__(config, save_dir)\n",
    "        self.cluster_id = cluster_id\n",
    "        \n",
    "    def train(self, client_data_list):\n",
    "        num_clients = len(client_data_list)\n",
    "\n",
    "        train_loss_list = []\n",
    "        test_acc_list = []\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        \n",
    "        optimizer = OPTIMIZER_LIST[self.config[\"optimizer\"]](self.model.parameters(), **self.config[\"optimizer_params\"])\n",
    "        #eff_num_workers = int(num_clients/(1 - 2*beta))\n",
    "        # if eff_num_workers > 0:\n",
    "        #     eff_batch_size = self.config[\"train_batch\"]/eff_num_workers\n",
    "        #     for i in range(num_clients):\n",
    "        #         client_data_list[i].trainloader.batch_size = eff_batch_size\n",
    "                \n",
    "        for iteration in tqdm(range(self.config[\"iterations\"])):\n",
    "            trmean_buffer = {}\n",
    "            for idx, param in self.model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    trmean_buffer[idx] = []\n",
    "            train_loss = 0\n",
    "            #optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            for client in client_data_list:\n",
    "                #if eff_num_workers>0:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                (X,Y) = client.sample_batch()\n",
    "                X = X.to(config[\"device\"])\n",
    "                Y = Y.to(config[\"device\"])\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                out = self.model(X)\n",
    "                loss = loss_func(out,Y)\n",
    "                loss.backward()\n",
    "                train_loss += loss.detach().cpu().numpy().item()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for idx, param in self.model.named_parameters():\n",
    "                        if param.requires_grad:\n",
    "                            trmean_buffer[idx].append(param.grad.clone())\n",
    "            train_loss = train_loss/num_clients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start_idx = 0\n",
    "            end_idx = num_clients\n",
    "\n",
    "\n",
    "            for idx, param in self.model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    sorted, _  = torch.sort(torch.stack(trmean_buffer[idx], dim=0), dim=0)\n",
    "                    new_grad = sorted[start_idx:end_idx,...].mean(dim=0)\n",
    "                    param.grad = new_grad\n",
    "                    trmean_buffer[idx] = []\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            test_acc = 0\n",
    "            for client_data in client_data_list:\n",
    "                test_acc += calc_acc(self.model, self.device, client_data, train=False)\n",
    "            test_acc = test_acc/num_clients\n",
    "            test_acc_list.append(test_acc)\n",
    "            self.model.train()\n",
    "            if iteration % self.config[\"save_freq\"] == 0 or iteration == self.config[\"iterations\"] - 1:\n",
    "                self.save_model_weights()\n",
    "                self.save_metrics(train_loss_list, test_acc_list, iteration)\n",
    "            if iteration % self.config[\"print_freq\"] == 0 or iteration == self.config[\"iterations\"] - 1:\n",
    "                print(\"Iteration : {} \\n , Train Loss : {} \\n, Test Acc : {} \\n\".format(iteration,  train_loss, test_acc))\n",
    "                \n",
    "        self.model.eval()\n",
    "        self.model.cpu()\n",
    "\n",
    "\n",
    "    def test(self, client_data_list):\n",
    "        self.load_model_weights()\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        test_acc = 0\n",
    "        for client_data in client_data_list:\n",
    "            test_acc += calc_acc(self.model, self.device, client_data, train=False)\n",
    "        self.model.cpu()\n",
    "        return test_acc\n",
    "\n",
    "\n",
    "def avg_acc(model_wts, client_data_list):\n",
    "    orig = model_wts[0]\n",
    "    if len(model_wts) > 0:\n",
    "        for wt in model_wts[1:]:\n",
    "            for key in orig.keys():\n",
    "                if orig[key].dtype == torch.float32:\n",
    "                    orig[key] += wt[key] \n",
    "        for key in orig.keys():\n",
    "            if orig[key].dtype == torch.float32:\n",
    "                orig[key] = orig[key]/len(model_wts)\n",
    "    model = SimpleCNN()\n",
    "    model.load_state_dict(orig)\n",
    "    model.to(memory_format = torch.channels_last).cuda()\n",
    "    test_acc = 0\n",
    "    for client_data in client_data_list:\n",
    "        test_acc += calc_acc(model, torch.device(\"cuda:0\"), client_data, train=False)\n",
    "    test_acc = test_acc/len(client_data_list)\n",
    "    return test_acc, orig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee799c64-6156-4c1e-b532-f2e902346952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cluster_map(num_clusters, client_list):\n",
    "    cluster_map = {}\n",
    "    for i in range(num_clusters):\n",
    "        cluster_map[i] = []\n",
    "    for i, _ in enumerate(client_list):\n",
    "        cluster_map[i%num_clusters].append(i)\n",
    "    return cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feb05559-fc13-4472-b44d-e12b1e079745",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_clusters\"] = 2\n",
    "cluster_map = init_cluster_map(config[\"num_clusters\"], config[\"selected_clients\"])\n",
    "  \n",
    "MODEL_LIST = {\"cnn\" : SimpleCNN}\n",
    "OPTIMIZER_LIST = {\"sgd\": optim.SGD, \"adam\": optim.Adam}\n",
    "LOSSES = {\"cross_entropy\": nn.CrossEntropyLoss()}\n",
    "# config[\"save_dir\"] = os.path.join(\"./results\")\n",
    "config[\"iterations\"] = 100\n",
    "config[\"optimizer_params\"] = {\"lr\":0.001}\n",
    "config[\"save_freq\"] = 2\n",
    "config[\"print_freq\"]  = 20\n",
    "config[\"model\"] = \"cnn\"\n",
    "config[\"optimizer\"] = \"adam\"\n",
    "config[\"loss_func\"] = \"cross_entropy\"\n",
    "#config[\"model_params\"] = {\"num_channels\": 1 , \"num_classes\"  : 62}\n",
    "config[\"model_params\"] = {}\n",
    "config[\"device\"] = torch.device(\"cuda:0\")\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for i in tqdm(range(len(config[\"selected_clients\"]))):\n",
    "#    client_trainers[i].train(client_loaders[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4d7ee0d-385c-4921-b8fa-9214b5e16493",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_rounds\"] = 1\n",
    "cluster_trainers = []\n",
    "for cluster_id in cluster_map.keys():\n",
    "    cluster_trainers.append(ClusterTrainer(config,\"\", cluster_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c04a5-04cf-4a03-8763-70cb17715211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbbf94ef-14f1-4934-98ad-432efb6925d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model, device, client_data, train,loss_func):\n",
    "    loader = client_data.trainloader if train else client_data.testloader\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tot_loss = 0\n",
    "    tot_num = 0\n",
    "    with torch.no_grad():\n",
    "        for (X,Y) in loader:\n",
    "            X = X.to(device)\n",
    "            out = model(X).detach().cpu()\n",
    "            loss = loss_func(out,Y).item()\n",
    "            tot_loss += loss\n",
    "            tot_num += Y.shape[0]\n",
    "    avg_loss = tot_loss/tot_num\n",
    "    return avg_loss\n",
    "def recluster(config, cluster_trainers, client_loaders):\n",
    "    new_map = {}\n",
    "    for i in range(len(cluster_trainers)):\n",
    "        new_map[i] = []\n",
    "    for client_id, client in enumerate(client_loaders):\n",
    "        best_loss = np.infty\n",
    "        best_cluster_idx = 0\n",
    "        for cluster_id, trainer in enumerate(cluster_trainers):\n",
    "            client_loss = calc_loss(trainer.model, config[\"device\"], client, train=True, loss_func = nn.CrossEntropyLoss())\n",
    "            if best_loss > client_loss:\n",
    "                best_loss = client_loss\n",
    "                best_cluster_idx = cluster_id\n",
    "        new_map[best_cluster_idx].append(client_id)\n",
    "    return new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b41ad930-70e4-4374-9d95-5b64a625f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▍                                          | 1/100 [00:04<07:49,  4.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 \n",
      " , Train Loss : 0.4045368963852525 \n",
      ", Test Acc : 39.59782028198242 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▊                                          | 2/100 [00:06<05:14,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▎                                         | 3/100 [00:09<04:57,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▋                                         | 4/100 [00:12<04:44,  2.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|██▏                                        | 5/100 [00:15<04:46,  3.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▌                                        | 6/100 [00:18<04:31,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|███                                        | 7/100 [00:22<04:53,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▍                                       | 8/100 [00:24<04:20,  2.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▊                                       | 9/100 [00:27<04:25,  2.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▏                                     | 10/100 [00:30<04:19,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▌                                     | 11/100 [00:37<06:28,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█████                                     | 12/100 [00:39<05:24,  3.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▍                                    | 13/100 [00:58<11:51,  8.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▉                                    | 14/100 [01:00<08:57,  6.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|██████▎                                   | 15/100 [01:14<12:07,  8.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▋                                   | 16/100 [01:17<09:46,  6.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|███████▏                                  | 17/100 [01:20<07:58,  5.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▌                                  | 18/100 [01:22<06:23,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▉                                  | 19/100 [01:26<05:57,  4.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████▍                                 | 20/100 [01:28<04:58,  3.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▊                                 | 21/100 [01:32<04:52,  3.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 20 \n",
      " , Train Loss : 0.4938193145208061 \n",
      ", Test Acc : 72.2472152709961 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|█████████▏                                | 22/100 [01:34<04:20,  3.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████▋                                | 23/100 [01:37<04:06,  3.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██████████                                | 24/100 [01:39<03:32,  2.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████▌                               | 25/100 [01:43<03:54,  3.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██████████▉                               | 26/100 [01:45<03:41,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|███████████▎                              | 27/100 [01:49<03:40,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▊                              | 28/100 [01:51<03:27,  2.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▏                             | 29/100 [01:54<03:28,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████▌                             | 30/100 [01:56<03:09,  2.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████                             | 31/100 [02:01<03:37,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████▍                            | 32/100 [02:03<03:13,  2.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████▊                            | 33/100 [02:06<03:16,  2.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|██████████████▎                           | 34/100 [02:09<03:11,  2.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|██████████████▋                           | 35/100 [02:12<03:10,  2.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████                           | 36/100 [02:14<03:02,  2.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███████████████▌                          | 37/100 [02:18<03:18,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███████████████▉                          | 38/100 [02:20<02:57,  2.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▍                         | 39/100 [02:23<02:55,  2.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████████████████▊                         | 40/100 [02:26<02:50,  2.83s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████▏                        | 41/100 [02:29<02:59,  3.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 40 \n",
      " , Train Loss : 0.33816722920164466 \n",
      ", Test Acc : 76.42537689208984 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|█████████████████▋                        | 42/100 [02:32<02:41,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████                        | 43/100 [02:35<02:54,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|██████████████████▍                       | 44/100 [02:37<02:35,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|██████████████████▉                       | 45/100 [02:41<02:38,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|███████████████████▎                      | 46/100 [02:44<02:42,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|███████████████████▋                      | 47/100 [02:47<02:36,  2.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████▏                     | 48/100 [02:49<02:20,  2.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████████████████████▌                     | 49/100 [02:53<02:34,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 50/100 [02:55<02:18,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▍                    | 51/100 [02:58<02:24,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████████████████████▊                    | 52/100 [03:01<02:19,  2.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▎                   | 53/100 [03:04<02:17,  2.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|██████████████████████▋                   | 54/100 [03:06<02:03,  2.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|███████████████████████                   | 55/100 [03:23<05:08,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|███████████████████████▌                  | 56/100 [03:25<04:05,  5.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|███████████████████████▉                  | 57/100 [03:29<03:38,  5.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|████████████████████████▎                 | 58/100 [03:32<03:04,  4.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|████████████████████████▊                 | 59/100 [03:45<04:46,  7.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▏                | 60/100 [03:47<03:37,  5.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|█████████████████████████▌                | 61/100 [03:51<03:22,  5.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 60 \n",
      " , Train Loss : 0.26741280127316713 \n",
      ", Test Acc : 76.99602508544922 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|██████████████████████████                | 62/100 [03:54<02:42,  4.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▍               | 63/100 [03:57<02:23,  3.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████████████████████████▉               | 64/100 [03:59<02:07,  3.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▎              | 65/100 [04:02<02:00,  3.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|███████████████████████████▋              | 66/100 [04:05<01:48,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▏             | 67/100 [04:09<01:47,  3.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████▌             | 68/100 [04:10<01:30,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|████████████████████████████▉             | 69/100 [04:13<01:26,  2.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████▍            | 70/100 [04:15<01:19,  2.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|█████████████████████████████▊            | 71/100 [04:18<01:19,  2.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|██████████████████████████████▏           | 72/100 [04:20<01:08,  2.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|██████████████████████████████▋           | 73/100 [04:24<01:14,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████           | 74/100 [04:26<01:07,  2.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████████████████████████████▌          | 75/100 [04:28<01:05,  2.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████████████████████████████▉          | 76/100 [04:31<01:04,  2.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|████████████████████████████████▎         | 77/100 [04:34<01:01,  2.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|████████████████████████████████▊         | 78/100 [04:36<00:55,  2.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████▏        | 79/100 [04:40<00:59,  2.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 80/100 [04:42<00:51,  2.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████        | 81/100 [04:45<00:54,  2.88s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 80 \n",
      " , Train Loss : 0.2550305149052292 \n",
      ", Test Acc : 77.4050521850586 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|██████████████████████████████████▍       | 82/100 [04:48<00:51,  2.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 83/100 [04:51<00:47,  2.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████████▎      | 84/100 [04:52<00:39,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|███████████████████████████████████▋      | 85/100 [04:56<00:41,  2.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████████      | 86/100 [04:58<00:36,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████████▌     | 87/100 [05:00<00:33,  2.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████▉     | 88/100 [05:03<00:29,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [05:05<00:27,  2.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████████████████████████████████▊    | 90/100 [05:07<00:22,  2.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [05:13<00:29,  3.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [05:15<00:22,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████████   | 93/100 [05:21<00:27,  3.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████████▍  | 94/100 [05:24<00:20,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [05:33<00:25,  5.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████████▎ | 96/100 [05:35<00:17,  4.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [05:40<00:13,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [05:41<00:07,  3.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [05:45<00:03,  3.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 100/100 [05:48<00:00,  3.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████████                      | 1/2 [05:48<05:48, 348.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 99 \n",
      " , Train Loss : 0.21574696619063616 \n",
      ", Test Acc : 78.14800262451172 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▍                                          | 1/100 [00:03<06:21,  3.85s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 \n",
      " , Train Loss : 0.4339981717057526 \n",
      ", Test Acc : 38.865013122558594 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▊                                          | 2/100 [00:05<04:31,  2.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▎                                         | 3/100 [00:08<04:01,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▋                                         | 4/100 [00:11<04:24,  2.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|██▏                                        | 5/100 [00:13<04:01,  2.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▌                                        | 6/100 [00:15<03:45,  2.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|███                                        | 7/100 [00:18<04:17,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▍                                       | 8/100 [00:20<03:48,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▊                                       | 9/100 [00:22<03:34,  2.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▏                                     | 10/100 [00:26<03:52,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▌                                     | 11/100 [00:28<03:54,  2.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█████                                     | 12/100 [00:30<03:37,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▍                                    | 13/100 [00:34<04:06,  2.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▉                                    | 14/100 [00:36<03:39,  2.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|██████▎                                   | 15/100 [00:38<03:29,  2.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▋                                   | 16/100 [00:42<04:02,  2.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|███████▏                                  | 17/100 [00:45<03:50,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▌                                  | 18/100 [00:47<03:35,  2.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▉                                  | 19/100 [00:51<04:02,  3.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████▍                                 | 20/100 [00:53<03:43,  2.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▊                                 | 21/100 [00:56<03:42,  2.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 20 \n",
      " , Train Loss : 0.5303092313697562 \n",
      ", Test Acc : 72.89054870605469 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|█████████▏                                | 22/100 [00:59<03:56,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████▋                                | 23/100 [01:02<03:42,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██████████                                | 24/100 [01:04<03:25,  2.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████▌                               | 25/100 [01:08<03:56,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██████████▉                               | 26/100 [01:11<03:40,  2.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|███████████▎                              | 27/100 [01:14<03:29,  2.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▊                              | 28/100 [01:17<03:43,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▏                             | 29/100 [01:20<03:29,  2.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████▌                             | 30/100 [01:22<03:12,  2.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████                             | 31/100 [01:27<03:45,  3.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████▍                            | 32/100 [01:29<03:21,  2.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████▊                            | 33/100 [01:31<03:10,  2.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|██████████████▎                           | 34/100 [01:35<03:23,  3.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|██████████████▋                           | 35/100 [01:38<03:09,  2.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████                           | 36/100 [01:40<02:57,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███████████████▌                          | 37/100 [01:44<03:21,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███████████████▉                          | 38/100 [01:46<02:59,  2.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▍                         | 39/100 [01:49<02:44,  2.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████████████████▊                         | 40/100 [01:52<02:49,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████▏                        | 41/100 [01:54<02:38,  2.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 40 \n",
      " , Train Loss : 0.36108798650093377 \n",
      ", Test Acc : 76.27527618408203 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|█████████████████▋                        | 42/100 [01:57<02:29,  2.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████                        | 43/100 [02:00<02:44,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|██████████████████▍                       | 44/100 [02:02<02:25,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|██████████████████▉                       | 45/100 [02:04<02:18,  2.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|███████████████████▎                      | 46/100 [02:08<02:36,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|███████████████████▋                      | 47/100 [02:11<02:25,  2.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████▏                     | 48/100 [02:13<02:16,  2.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████████████████████▌                     | 49/100 [02:17<02:36,  3.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 50/100 [02:19<02:18,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▍                    | 51/100 [02:21<02:09,  2.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████████████████████▊                    | 52/100 [02:24<02:12,  2.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▎                   | 53/100 [02:27<02:01,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|██████████████████████▋                   | 54/100 [02:29<01:54,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|███████████████████████                   | 55/100 [02:32<02:07,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|███████████████████████▌                  | 56/100 [02:35<01:55,  2.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|███████████████████████▉                  | 57/100 [02:37<01:46,  2.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|████████████████████████▎                 | 58/100 [02:40<01:51,  2.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|████████████████████████▊                 | 59/100 [02:42<01:41,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▏                | 60/100 [02:44<01:34,  2.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|█████████████████████████▌                | 61/100 [02:48<01:50,  2.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 60 \n",
      " , Train Loss : 0.33060413831844926 \n",
      ", Test Acc : 75.5167236328125 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|██████████████████████████                | 62/100 [02:50<01:39,  2.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▍               | 63/100 [02:52<01:35,  2.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████████████████████████▉               | 64/100 [02:56<01:44,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▎              | 65/100 [02:59<01:37,  2.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|███████████████████████████▋              | 66/100 [03:01<01:32,  2.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▏             | 67/100 [03:05<01:43,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████▌             | 68/100 [03:08<01:32,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|████████████████████████████▉             | 69/100 [03:10<01:26,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████▍            | 70/100 [03:14<01:30,  3.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|█████████████████████████████▊            | 71/100 [03:17<01:25,  2.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|██████████████████████████████▏           | 72/100 [03:19<01:17,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|██████████████████████████████▋           | 73/100 [03:23<01:25,  3.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████           | 74/100 [03:25<01:15,  2.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████████████████████████████▌          | 75/100 [03:28<01:10,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████████████████████████████▉          | 76/100 [03:32<01:15,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|████████████████████████████████▎         | 77/100 [03:34<01:07,  2.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|████████████████████████████████▊         | 78/100 [03:37<01:00,  2.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████▏        | 79/100 [03:41<01:05,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 80/100 [03:43<00:57,  2.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████        | 81/100 [03:46<00:53,  2.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 80 \n",
      " , Train Loss : 0.2818481170106679 \n",
      ", Test Acc : 76.810791015625 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|██████████████████████████████████▍       | 82/100 [03:49<00:54,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 83/100 [03:52<00:48,  2.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████████▎      | 84/100 [03:54<00:42,  2.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|███████████████████████████████████▋      | 85/100 [03:58<00:46,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████████      | 86/100 [04:01<00:41,  2.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████████▌     | 87/100 [04:03<00:36,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████▉     | 88/100 [04:07<00:36,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [04:09<00:31,  2.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████████████████████████████████▊    | 90/100 [04:11<00:26,  2.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [04:15<00:27,  3.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [04:18<00:22,  2.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████████   | 93/100 [04:20<00:19,  2.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████████▍  | 94/100 [04:24<00:17,  2.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [04:26<00:14,  2.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████████▎ | 96/100 [04:29<00:10,  2.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [04:33<00:09,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [04:35<00:05,  2.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [04:38<00:02,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████████| 100/100 [04:41<00:00,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████| 2/2 [10:30<00:00, 315.42s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████| 1/1 [10:40<00:00, 640.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 99 \n",
      " , Train Loss : 0.21598578715929762 \n",
      ", Test Acc : 77.58863067626953 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for round_idx in tqdm(range(config[\"num_rounds\"])):\n",
    "    cluster_map = recluster(config, cluster_trainers, client_loaders)\n",
    "\n",
    "    with open(os.path.join(config[\"results_dir\"],\"round_{}\".format(round_idx), \"cluster_maps.pkl\"), 'wb') as handle:\n",
    "            pickle.dump(cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    for cluster_id, cluster_clients in tqdm(cluster_map.items()):\n",
    "        cluster_clients = [client_loaders[i] for i in cluster_map[cluster_id]]\n",
    "        cluster_trainers[cluster_id].save_dir = os.path.join(config['results_dir'], \"round_{}\".format(round_idx), \"cluster_{}\".format(cluster_id))\n",
    "        cluster_trainers[cluster_id].train(cluster_clients)\n",
    "    if round_idx == config[\"num_rounds\"]-1:\n",
    "        with open(os.path.join(config[\"results_dir\"], \"final_cluster_map.pkl\"), 'wb') as handle:\n",
    "            pickle.dump(cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e003c4e-d89d-4b52-a2cb-11818c2752b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [2, 8, 10, 12, 14, 16, 17, 18, 22, 24, 29, 30],\n",
       " 1: [0, 1, 3, 4, 5, 6, 7, 9, 11, 13, 15, 19, 20, 21, 23, 25, 26, 27, 28, 31]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe304423-0067-4a52-bba2-b0b9e64d6de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057ac17-a4d6-43a0-a7fb-d0fdc2c4ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for new in tqdm(range(config[\"refine_steps\"])):\n",
    "    refine_step = 1\n",
    "    beta = 0.2\n",
    "    cluster_trainers = []\n",
    "    for cluster_id in tqdm(cluster_map.keys()):\n",
    "        cluster_clients = [client_loaders[i] for i in cluster_map[cluster_id]]\n",
    "        cluster_trainer = ClusterTrainer(config, os.path.join(config['results_dir'], \"refine_{}\".format(refine_step), \"cluster_{}\".format(cluster_id)), cluster_id)\n",
    "        cluster_trainer.train(cluster_clients)\n",
    "        cluster_trainers.append(cluster_trainer)\n",
    "    with open(os.path.join(config[\"results_dir\"],\"refine_{}\".format(refine_step), \"cluster_maps.pkl\"), 'wb') as handle:\n",
    "        pickle.dump(cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    cluster_map_recluster = {}\n",
    "    for key in cluster_map.keys():\n",
    "        cluster_map_recluster[key] = []\n",
    "\n",
    "    for i in tqdm(range(config[\"num_clients\"])):\n",
    "        w_node = client_trainers[i].model.state_dict()\n",
    "        norm_diff = np.infty\n",
    "        new_cluster_id = 0\n",
    "        for cluster_id in cluster_map.keys():\n",
    "            w_cluster = cluster_trainers[cluster_id].model.state_dict()\n",
    "            curr_norm_diff = model_weights_diff(w_node, w_cluster)\n",
    "            if norm_diff > curr_norm_diff:\n",
    "                new_cluster_id = cluster_id\n",
    "                norm_diff = curr_norm_diff\n",
    "        \n",
    "        cluster_map_recluster[new_cluster_id].append(i)\n",
    "    keys = list(cluster_map_recluster.keys()).copy()\n",
    "    for key in keys:\n",
    "        if len(cluster_map_recluster[key]) == 0:\n",
    "            cluster_map_recluster.pop(key)\n",
    "    cluster_map = cluster_map_recluster\n",
    "\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(cluster_map.keys())\n",
    "\n",
    "    all_pairs = list(itertools.combinations(cluster_map.keys(),2))\n",
    "    for pair in tqdm(all_pairs):\n",
    "        w_1  = cluster_trainers[pair[0]].model.state_dict()\n",
    "        w_2 = cluster_trainers[pair[1]].model.state_dict()\n",
    "        norm_diff = model_weights_diff(w_1, w_2)\n",
    "        if norm_diff < thresh:\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    G = G.to_undirected()\n",
    "    clustering = []        \n",
    "    correlation_clustering(G)\n",
    "    merge_clusters = [cluster  for cluster in clustering if len(cluster) > 0]\n",
    "    \n",
    "    #merge_cluster_map = {i: clusters[i] for i in range(len(clusters))}\n",
    "    #clusters = list(nx.algorithms.clique.enumerate_all_cliques(G))\n",
    "    cluster_map_new = {}\n",
    "    for i in range(len(merge_clusters)):\n",
    "        cluster_map_new[i] = []\n",
    "        for j in merge_clusters[i]:\n",
    "            cluster_map_new[i] += cluster_map[j]\n",
    "    cluster_map = cluster_map_new\n",
    "    test_acc = 0\n",
    "    for cluster_id in tqdm(cluster_map.keys()):\n",
    "        cluster_clients = [client_loaders[i] for i in cluster_map[cluster_id]]\n",
    "        model_wts = [cluster_trainers[j].model.state_dict() for j in merge_clusters[cluster_id]]\n",
    "        test_acc_cluster, model_avg_wt =avg_acc(model_wts,cluster_clients)\n",
    "        torch.save(model_avg_wt, os.path.join(config['results_dir'], \"refine_{}\".format(refine_step), \"merged_cluster_{}.pth\".format(cluster_id)))\n",
    "        test_acc += test_acc_cluster\n",
    "    test_acc = test_acc/len(cluster_map.keys())\n",
    "    torch.save(test_acc, os.path.join(config['results_dir'], \"refine_{}\".format(refine_step), \"avg_acc.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4530fe0-2854-495b-9f18-f5ae8e4d6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trainer = GlobalTrainer(config, os.path.join(config[\"results_dir\"], \"global\"))\n",
    "global_trainer.train(client_loaders)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
